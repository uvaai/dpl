# Data Acquisition

Data acquisition is the process of collecting or sampling data from a source. In the context of Computer Science, this often means downloading a database from a source on the internet. However, sometimes a website might not give an option to download the data, but does present it (think of websites like Wikipedia or Twitter).

Web scraping is a way of extracting data from websites. While this process could be done manually by reading information on a website, and then copying that information to a file, it is usually done through the use of software. Scraping can be a valuable tool for extracting data.

Getting information from websites require a decent understanding of HTML, so we start this module with the creation of a simple website. Then we'll focus on scraping data from the Internet Movie Database (IMDb). This website is an online database of information related to films, programs, home videos, games, and streaming content. It includes information like cast, production crew and personal biographies, plot summaries, trivia, ratings, and fan and critic reviews.

We will continue with the data we gather in this module in the next module. After transforming and visualizing the data we will focus on web crawling. Web crawling is a technique where a program visits multiple webpages and scrapes each page for information. An example of where this technique is used, is by search engines to update their web content. In our case, we will use it to scrape information from different pages.

**There will be no grade for this module, but you will get feedback. This feedback can then be used to improve your code for the next module, in which you will continue with the data you scrape this week.**
